<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Expanding Picking Actions for Time-Efficient Online 3D Bin Packing">
  <meta name="keywords" content="BinPacking, STEP, Time-Efficient">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Expanding Picking Actions for Time-Efficient Online 3D Bin Packing</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QHEMNFW1LR"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-QHEMNFW1LR');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script src="./static/js/pareto-chart.js"></script>
</head>
<body>




<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Expanding Picking Actions for Time-Efficient Online 3D Bin Packing</h1>
          <!-- TODO: ADD LATER -->
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">Keunhong Park</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Utkarsh Sinha</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Jonathan T. Barron</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Google Research</span>
          </div> -->

          <!-- TODO: ADD LATER -->
          <!-- <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!--
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!--
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <!--
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!--
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%" controls>
        <source src="./static/videos/hero-video-bpp-process-pareto.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        STEP (Space-Time Efficient Packing) explicitly accounts for the time required to reorient and handle packages, jointly reasoning over space utilization and operational time.
        STEP learns to balance speed and packing efficiency through optimized item and face selection. This demo shows packing behaviors under different space and time weights, ranging from fast execution to dense packing.
      </h2>
    </div>
  </div>
</section>

<!-- TODO: CHECK AND REMOVE-->
 <!--
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
-->



<section class="section">
  <div class="container is-max-desktop">
    <!-- Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
      </div>
    </div>
    
    <div class="columns is-vcentered">
      <!-- Image column -->
      <div class="column is-half">
        <figure class="image">
          <img src="./static/images/different-faces-box.png" alt="Different faces of a box showing various grasp orientations">
          <figcaption class="has-text-centered is-size-9 has-text-grey" style="margin-top: 0.5rem;">
            Face selection matters: different grasp directions achieve better packing through reorientation, but also affects how quickly and safely a robot can pick and place items.
          </figcaption>
        </figure>
      </div>
      
      <!-- Text column -->
      <div class="column is-half">
        <div class="content has-text-justified">
          <p>
            Robotic bin packing in warehouses demands a careful balance between two often conflicting goals: maximizing space utilization and minimizing operational time.
            STEP (Space-Time Efficient Packing) addresses this trade-off by learning to select not just which item to pack, but also which face to grasp it from. Each choice influences both the quality of placement and the time required to execute it. Our method uses a preference-conditioned multi-objective policy that dynamically weighs packing efficiency against time cost, adapting its strategy based on user-specified preferences.
            While STEP is trained and evaluated in simulation, its design is grounded in real-world robotic constraints which include grasp direction, object surface type, and the impact of transport failures. These factors can directly be abstracted into the time model used during training, ensuring the policy reflects practical deployment conditions.
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Real-world Insights. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Real-World Insights</h2>
      </div>
    </div>
    <!-- Grasp Sides -->
    <div class="columns is-vcentered">
      <!-- Video column -->
      <div class="column is-half">
        <video autoplay muted loop playsinline height="100%" controls>
          <source src="./static/videos/robot-grasp-sides.mp4" type="video/mp4">
        </video>
      </div>
      
      <!-- Text column -->
      <div class="column is-half">
        <div class="content has-text-justified">
          <p>
          Different grasp directions directly affect how quickly and reliably the robot can pick an item:
          <ul>
            <li>Top grasps are often faster.</li>
            <li>Side grasps can be slower.</li>
            <li>A robot may also reorient an item by first picking from the top face, placing it in a new orientation, and then picking it again.</li>
          </ul>
          <p>
            Importantly, reorientation times are not universal - they depend on the specific robotic setup, hardware, and motion planning constraints. 
            STEP abstracts all of these strategies into a unified time cost, which is explicitly taken into account when planning for bin packing. 
            Each graspable face is treated as a separate candidate with its own time cost.
          </p>
        </p>
        </div>
      </div>
    </div>
    <!-- Trajectory Duration -->
    <div class="columns is-vcentered">
      
      <!-- Text column -->
      <div class="column is-half">
        <div class="content has-text-justified">
          <p>
            Transport speed also affects reliability:
            <ul>
              <li>If a box with an uneven or difficult surface is moved too quickly, the suction cups may lose contact and the package can drop.</li>
              <li>Slowing down avoids failure, but increases operation time.</li>
            </ul>
            <p>
              The suction-cup dynamics with the box surface directly influence the safety and reliability of transport, and these effects depend on the specific setup and item. 
              Since STEP explicitly optimizes for operational time, such behaviors are abstracted into face-dependent time penalties during bin packing. 
              In training, we defined three surface categories - smooth, plastic-wrapped, and package-labeled â€” and randomly assigned each box face to one of these categories in simulation, with each category given a fixed time penalty.
            </p>
          </p>
        </div>
      </div>

      <!-- Video column -->
      <div class="column is-half">
        <video autoplay muted loop playsinline height="100%" controls>
          <source src="./static/videos/robot-trajectory-duration.mp4" type="video/mp4">
        </video>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- System Architecture. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">System Architecture</h2>
      </div>
    </div>
    
    <!-- Image section -->
    <div class="columns is-centered">
      <div class="column is-four-quarters">
        <figure class="image">
          <img src="./static/images/system_architecture.png" alt="System Architecture">
        </figure>
      </div>
    </div>
    
    <!-- Text section -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            STEP frames bin packing as a multi-candidate, multi-objective selection problem that aims to balance space utilization and operational time. Each graspable face of each item in the buffer is treated as a distinct candidate in the selection process.
          </p>
          <p>The policy receives as input:</p>
          <ul>
            <li>Current bin state (as empty maximal spaces)</li> 
            <li>Item-face pairs with their selected placement position and rotation</li>
            <li>Estimated operational time for each item-face pair</li>
            <li>Preference vector specifying space-time trade-off</li>
          </ul>
          <p>
           The Transformer-Select module processes the bin and item-face features to produce embeddings.

          </p><p>These embeddings are then combined with the preference vector in the actor and critic heads to compute scores over all item-face candidates.

          </p><p>The highest-scoring candidate is selected; the bin state is updated, and a new item enters the buffer.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Pareto Front Section. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">From Space to Time: A Pareto Perspective</h2>
      </div>
    </div>

    <!-- Text section -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Space utilization and operational time are inherently conflicting in robotic bin packing:
          </p>
          <ul>
            <li>Achieving denser packing often requires extra reorientations and longer actions.</li>
            <li>Minimizing execution time, in contrast, can lead to wasted bin space.</li>
          </ul>
          <p>
            STEP resolves this conflict through a <strong>preference vector</strong>, which defines how much weight to place on each objective. By tuning this vector, the policy can prioritize space efficiency, time efficiency, or a balance of both within a single framework.
          </p>
          <p>
            The <strong>Pareto front</strong> below illustrates the achievable trade-offs. <strong>STEP-n</strong> denotes a policy with <em>n</em> items in the buffer in the semi-online setting. The figure shows results for buffers of size 1, 3, and 5, each evaluated across preference vectors ranging from 0 to 1 for both objectives. Larger buffers provide more candidate choices and improve space utilization, while the preference vector governs how the trade-off between space and time is considered, respectively.
          </p>
        </div>
      </div>
    </div>

    <!-- Image section -->
    <div class="columns is-centered">
      <div class="column is-four-quarters">
        <div class="has-text-centered" style="margin-bottom: 5px;">
          <span class="tag is-info is-light is-medium">
            <i class="fas fa-mouse-pointer" style="margin-right: 8px;"></i>
            Hover over points to see detailed values
          </span>
        </div>
        <div id="pareto-chart" style="width: 100%; height: 600px;"></div>
      </div>
    </div>

  </div>
</section>




<!-- <section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
        
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
          
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>

        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>


    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>


  </div>
</section> -->


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <video id="teaser" autoplay muted loop playsinline height="100%" controls>
          <source src="./static/videos/full-submitted-video-720.mp4"
                type="video/mp4">
        </video>
      </div>
    </div>
    <!--/ Paper video. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robotic bin packing is widely deployed in warehouse automation, with current systems achieving robust performance through heuristic and learning-based strategies. These systems must balance compact placement with rapid execution, where actions such as selecting alternative items or reorienting them can improve space utilization but introduce additional time. We propose a selection-based formulation that explicitly reasons over this trade-off: at each step, the robot evaluates multiple candidate actions, weighing expected packing benefit against estimated operational time. This enables time-aware strategies that selectively accept increased operational time when it yields meaningful spatial improvements. Our method, STEP (Space-Time Efficient Packing), uses a Transformer-based policy conditioned on dynamic preferences,  and allows generalization across candidate set sizes and integration with standard placement modules. It achieves higher packing density without compromising operational time.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        This website template is borrowed from <a href="https://nerfies.github.io/">nerfies.github.io</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>